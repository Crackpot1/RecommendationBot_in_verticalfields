#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¯åŠ¨è„šæœ¬ - è·³è¿‡PDFå¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰å‘é‡åº“
"""

import os
import socket
import json
import threading
from dataclasses import dataclass
from config import setup_environment, get_pinecone_config, get_server_config, get_model_config
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from pinecone import Pinecone
from langchain_pinecone import PineconeVectorStore
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory

# è®¾ç½®ç¯å¢ƒå˜é‡
setup_environment()

@dataclass
class UserInfo:
    Name: str
    Query: str
    Answer: str = ""

# Concrete implementation of BaseChatMessageHistory
class SimpleChatMessageHistory(BaseChatMessageHistory):
    def __init__(self):
        self.messages = []

    def add_message(self, message):
        self.messages.append(message)

    def get_messages(self):
        return self.messages

    def clear(self):
        self.messages.clear()

# è·å–é…ç½®
server_config = get_server_config()
host, port = server_config["host"], server_config["port"]

# å…¨å±€å˜é‡
vector_store = None
llm = None
embeddings_model = None
_retrieval_chain = None

def quick_initialize():
    """å¿«é€Ÿåˆå§‹åŒ– - è·³è¿‡PDFå¤„ç†"""
    global vector_store, llm, embeddings_model
    
    print("ğŸš€ å¿«é€Ÿåˆå§‹åŒ–ä¸­...")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹ (å¿«é€Ÿ)
    print("1. åˆå§‹åŒ–æ¨¡å‹...")
    model_config = get_model_config()
    
    # åˆå§‹åŒ–LLM
    llm = Ollama(
        model=model_config["llm_model"],
        base_url=model_config["ollama_base_url"]
    )
    print("   âœ… LLMåˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ–Embeddingæ¨¡å‹
    embeddings_model = OllamaEmbeddings(model=model_config["embedding_model"])
    print("   âœ… Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 2. è¿æ¥ç°æœ‰å‘é‡åº“ (å¿«é€Ÿ)
    print("2. è¿æ¥å‘é‡åº“...")
    try:
        pinecone_config = get_pinecone_config()
        pc = Pinecone(api_key=pinecone_config["api_key"])
        
        # ç›´æ¥è¿æ¥åˆ°ç°æœ‰çš„å‘é‡åº“
        vector_store = PineconeVectorStore.from_existing_index(
            index_name="recipe-index",
            embedding=embeddings_model
        )
        print("   âœ… å‘é‡åº“è¿æ¥æˆåŠŸ")
        
    except Exception as e:
        print(f"   âŒ å‘é‡åº“è¿æ¥å¤±è´¥: {e}")
        print("   ğŸ’¡ è¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡å®Œæ•´åˆå§‹åŒ–ï¼Œæˆ–è€…è¿è¡Œ: python recipechatbot.py")
        return False
    
    print("âœ… å¿«é€Ÿåˆå§‹åŒ–å®Œæˆï¼")
    return True

# Define prompts
system_prompt = (
    "You are an assistant specialized in providing personalized cooking recipe recommendations. "
    "You cater to various user preferences such as dietary restrictions, cuisine types, available ingredients, and cooking time. "
    "Answer the user's questions based on the context provided from the vector database. "
    "If the context does not have relevant information,inform the user that the question is out of scope or the information is not available. Do NOT answer the question out of the vector database."
    "<think></think>\n\n"
    "{context}"
)

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is."),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

def get_retrieval_chain(vector_store):
    global _retrieval_chain
    if _retrieval_chain is None:
        print("æ­£åœ¨åˆ›å»ºæ£€ç´¢é“¾...")
        retriever = create_history_aware_retriever(
            llm=llm, 
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}), 
            prompt=contextualize_q_prompt
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        _retrieval_chain = create_retrieval_chain(
            retriever=retriever,
            combine_docs_chain=question_answer_chain
        )
        print("æ£€ç´¢é“¾åˆ›å»ºå®Œæˆ")
    return _retrieval_chain

# å­˜å‚¨ä¼šè¯å†å²
store = {}

def get_session_history(session_id: str) -> SimpleChatMessageHistory:
    if session_id not in store:
        store[session_id] = SimpleChatMessageHistory()
    return store[session_id]

def get_answer(query):
    if vector_store is None:
        return {"answer": "é”™è¯¯: å‘é‡åº“æœªåˆå§‹åŒ–"}
    
    try:
        retrieval_chain = get_retrieval_chain(vector_store)
        session_id = "session_id"
        history = get_session_history(session_id)

        answer = retrieval_chain.invoke({"input": query, "chat_history": history.get_messages()})

        # è·å–åŸå§‹æ£€ç´¢ç»“æœåŠåˆ†æ•°
        raw_results = vector_store.similarity_search_with_score(query, k=5)
        docs, scores = zip(*raw_results)

        # æ‰‹åŠ¨è¿‡æ»¤ä½åˆ†ç»“æœ
        if max(scores) < 0.5375:
            return {"answer": "Your question is beyond the scope of my current knowledge. I can provide you with some recipe advice..."}

        return answer
    except Exception as e:
        print(f"è·å–ç­”æ¡ˆæ—¶å‡ºé”™: {e}")
        return {"answer": f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"}

def send_user_info(client_socket, user_info):
    try:
        json_data = json.dumps(user_info.__dict__)
        json_data += '\n'
        client_socket.sendall(json_data.encode())
        print("å·²å‘é€å›å®¢æˆ·ç«¯")
    except Exception as e:
        print(f"å‘é€æ•°æ®æ—¶å‡ºé”™: {e}")

def handle_client(client_socket):
    receive_thread = threading.Thread(target=receive_user_info, args=(client_socket,))
    receive_thread.start()

def receive_user_info(client_socket):
    while True:
        try:
            received_data = client_socket.recv(4096).decode()
            if not received_data:
                break
            
            user_data = json.loads(received_data)
            user_info = UserInfo(**user_data)
            print("æ”¶åˆ°Unityä¿¡æ¯:", user_info)

            answer = get_answer(user_info.Query)
            user_info.Answer = answer["answer"] if isinstance(answer, dict) else str(answer)
            
            print("ç”Ÿæˆçš„ç­”æ¡ˆ:", user_info.Answer)
            send_user_info(client_socket, user_info)
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
        except Exception as e:
            print(f"æ¥æ”¶æ•°æ®æ—¶å‡ºé”™: {e}")
            break
    
    client_socket.close()

def start_server():
    # å¿«é€Ÿåˆå§‹åŒ–
    if not quick_initialize():
        print("å¿«é€Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡å™¨")
        return
    
    # åˆ›å»ºTCP socket
    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    
    try:
        server_socket.bind((host, port))
        server_socket.listen(5)
        print(f"ğŸš€ æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼æ­£åœ¨ç›‘å¬ {host}:{port}")
        print("ğŸ’¡ è¿™æ˜¯å¿«é€Ÿå¯åŠ¨æ¨¡å¼ï¼Œè·³è¿‡PDFå¤„ç†æ­¥éª¤")
        
        while True:
            client_socket, addr = server_socket.accept()
            print(f"âœ… å®¢æˆ·ç«¯è¿æ¥: {addr}")

            client_thread = threading.Thread(target=handle_client, args=(client_socket,))
            client_thread.daemon = True
            client_thread.start()
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨æ­£åœ¨å…³é—­...")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}")
    finally:
        server_socket.close()

if __name__ == "__main__":
    print("âš¡ å¿«é€Ÿå¯åŠ¨æ¨¡å¼")
    print("=" * 50)
    print("æ­¤æ¨¡å¼å°†è·³è¿‡PDFå¤„ç†ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰å‘é‡åº“")
    print("å¦‚æœå‘é‡åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ: python recipechatbot.py")
    print("=" * 50)
    
    start_server() 